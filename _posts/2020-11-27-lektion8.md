---
title: "Lektion VIII"
date: 2020-11-27
header:
  image: /assets/images/BAIN8.png
  teaser: "/assets/images/BAIN8.png"
---
## Metadaten modellieren und Schnittstellen nutzen: Teil III
Zum Abschluss des Metadaten-Teils dieser Vorlesung haben wir noch kurz über weitere Tools zur Metadatentransformation bzw. über Alternativen zu OpenRefine gesprochen. OpenRefine haben wir in den vergangenen zwei Lehreinheiten bereits kennengelernt. Ein grosser Vorteil dieses Programms liegt natürlich in der grafischen Benutzeroberfläche, in der die Ergebnisse unserer eingegebenen Transformationen direkt sichtbar werden. Bei der Datenanreicherung bzw. Reconciliation liegt auch der Schwerpunkt des Programms. Wir haben auch ein wenig mit der Skriptsprache GREL experimentiert, weitere, von uns nicht verwendete Skriptsprachen wären Jython oder Clojure. 

Es gibt im dem Sinn eigtentlich keine "one size fits all"-Lösung für die Metadatenmodellierung. Es kommt immer auf den konkreten Anwendungsfall drauf an und nicht zuletzt auch darauf, welche Kenntnisse (z.B. der entsprechenden Programmiersprachen) bereits vorhanden sind. 

Es gibt neben OpenRefine jedoch noch andere Programme für die Modellierung von Metadaten. Eines davon, [MarcEdit](https://marcedit.reeset.net) haben wir im Kurs auch bereits kennen gelernt (mehr dazu: s. [Lektion VI](https://leabaechli.github.io/bain/lektion6/)). 

Ein weiteres Programm ist [Catmandu](https://librecat.org). Catmandu bietet eine Auswahl von Perl-Befehlen, um den Datenimport, das Retrieval und auch die Datentransformation zu steuern. 

Nicht zuletzt wurde auch [Metafacture](https://github.com/metafacture/metafacture-core), das auf Java basiert, angesprochen. Metafacture besteht aus den Teilen Framework, Morph und Flux. Das Framework bietet mit den Interfaces und den Klassen die Grundlage für die Metadatenmodullierung. Metafacture Morph - kurz: Metamorph - wird für die Datentransformation verwendet. Und Flux ist die dazugehörige Skriptsprache.

### JSON-APIs
Heutzutage ist es gang und gäbe, dass APIs Daten nicht in XML oder OAI-PMH liefern, sondern im Datenformat JSON (JavaScript Object Notation). JSON ist ebenso wie XML gut menschlich wie auch maschinell les- und verarbeitbar und lässt sich direkt im Browser anschauen. Ein Beispiel dafür wäre [lobid-gnd](https://lobid.org/gnd/api), wo sich Daten direkt als JSON abrufen lassen. 

Als Tool für einen Scraper wurde scrAPIr(https://scrapir.org) genannt, welche das Ziel hat, Daten von Webseiten gezielt zu scrapen. Dabei wird nicht zwingend JSON verwendet, sondern die verwendeten APIs der entsprechenden Webseiten - bei modernen Webseiten also v.a. JSON. 

### LIDO
Als kleiner Exkurs wurde noch der Metadatenstandard LIDO vorgestellt. Diese praktische Abkürzung steht für das doch etwas umfangreichere "Lightweight Information Describing Objects". LIDO ist ein dek Linked Open Data-Paradigma folgender, auf dem [CIDOC Conceptual Reference Model (CRM)](http://cidoc.mini.icom.museum/) basierender XML-Standard und wird zur Beschreibung von Kulturobjekten verwendet. [CIDOC CRM](https://de.wikipedia.org/wiki/CIDOC_Conceptual_Reference_Model) ist eine Ontologie, welche Begriffe und Informationen rund um das Kulturerbe umfasst. Diese dient dem kontrollierten Austausch von Informationen und wird v.a. von Museen, Archiven und Bibliotheken verwendet. 

Bei LIDO ist es dann so, dass die URI für bestimmte Konzepte und Relationen aus der Ontologie von CIDOC CRM bestimmt wird und LIDO sich auch bei der Terminologie an CIDOC CRM orientiert. Besonders an LIDO ist es, dass Objekte ereignis-zentriert beschrieben werden, Ereignisse so also als Entitäten erfasst werden. Dies hat den Nachteil, dass eine verlustfreie Transformation in andere Formate meist schwierig ist, da andere Metadatenstandards dies nicht so handhaben. 

Die LIDO Struktur umfasst deskriptive sowie administrative Metadaten sowie die Untershceidung zwischen *display elements* und *index elements*. 

* deskriptive Metadaten:
  * Identifikation (Titel/Name, Beschreibung, Maße, etc.)
  * Klassifikation (Art, Gattung, Form, etc.)
  * **Ereignisse** (Herstellung, Bearbeitung, Besitzwechsel, Restaurierung, etc.)
  * Relationen (Objekte, Personen, Orte, etc.)
* administrative Metadaten:
  * Rechte (Objekt, Datensatz, Nutzung, Verbreitung, etc.)
  * Datensatz (Identifikation, Urheber, etc.)
  * Ressourcen (Digitalisat, Nachweis, etc.)

## Suchmaschinen und Discovery-Systeme: Teil I
Weil sich unser Zeitplan etwas nach hinten verschoben hat, kamen wir heute nicht sehr weit mit den Discovery-Systemen. Wir haben uns alle gemeinsam die Software [VuFind](https://vufind.org) (Version 7.0.1) heruntergeladen und auf der virtuellen Maschine installiert. Die Installation war nicht ganz ohne ihre Tücken, hat aber dann schlussendlich geklappt. Dass wir die Installation Schritt für Schritt im Plenum durchgeführt haben, hat mir nochmals ein bisschen geholfen, die verschiedenen Funktionen der Linux-Shell noch ein bisschen besser nachvollziehen zu können. 

### VuFind
[VuFind]() ist ein Open-Source-Programm, welches 2010 von der Villanova Universität veröffentlicht wurde. Dabei handelt es sich um ein Discovery-Bibliothekssystem - es bietet den Nutzenden also mehr als ein traditioneller OPAC. Wie wir schon bei der Installation bzw. Konfiguration der Software bemerkt haben, bietet VuFind sehr viele verschiedene Konfigurationsmöglichkeiten für diverse Anwendefälle. 

Da wir in der nächsten Sitzung noch weiter über Discovery-Systeme und VuFind sprechen werden, belasse ich das hier mal mit dieser Kurzinfo zu VuFind. Mehr also im nächsten Eintrag... 
