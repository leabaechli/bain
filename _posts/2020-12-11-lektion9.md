---
title: "Lektion IX"
date: 2020-12-11
header:
  image: /assets/images/BAIN9.png
  teaser: "/assets/images/BAIN9.png"
---

## Suchmaschinen und Discovery-Systeme: Teil II
Heute geht's weiter mit Suchmaschinen und Discovery-Systemen, in unserem Fall also mit VuFind. Wir sind schon fast am Ende des Kurses angelangt und jetzt brauchen wir alle noch ein bisschen Durchhaltevermögen, damit wir auch den Hilfe-das-Semester-ist-bald-fertig-Workload meistern. In diesem Sinne - los geht's mit dem, was wir in dieser Lektion gelernt haben! 

### Solr
Solr ist uns nach fast einem ganzen Semester Bibliotheks- und Archivinformatik wohl allen ein Begriff. Bisher wurde Solr aber nur am Rande erwähnt. Heute folgt eine Schnellbleiche dazu. 

[Solr](https://lucene.apache.org/solr/) ist eine open-source Suchmaschine, die in verschiedenen Anwendungsbereichen weit verbreitet ist und mittlerweile als Industriestandard gilt. Sie dient z.B. beim Discoverysystem VuFind als Suchmaschinen-Basis, wird aber auch von kommerziellen Produkten von Anbietern wie ExLibris verwendet. 

Solr selbst hat nicht wirklich eine Suchoberfläche. Die gibt es entweder integriert in die Adminoberfläche oder als Demo, die ist aber völlig ungestaltet und wirklich nur zu Testzwecken gedacht. Moderne Usability-Anforderungen werden hier wohl nicht erfüllt. 

Aber wie funktioniert Solr denn? 

[Solr](https://en.wikipedia.org/wiki/Apache_Solr) sucht mittels Volltextsuche. Damit die Dokumente in einer Datenbank per Volltext durchsuchbar sind, sind mehrere Schritte nötig. Zuerst werden alle Dokumente indexiert, also in ein maschinell lesbares Format konvertiert. Dann folgt die Suchanfrage des Nutzers, welche durch die lexikalische Suche von Solr nicht 1:1 übernommen wird, sondern z.B. auf Grundformen reduziert wird. Diese Suchanfrage wird in einem nächsten Schritt namens Mapping auf die in der Datenbank gelagerten Dokumente gemappt und so nach Treffern gesucht. Zum Schluss folgt dann noch das Ranking, das nach Relevanz passiert - dazu später noch etwas mehr. 

Es emppfiehlt sich bei Solr, vor dem Datenimport in einem Schema festzulegen, welche Datentypen und Felder überhaupt gewünscht sind. Es gibt zwar die Möglichkeit, ohne Schema (schemaless) zu importieren, da fehlt dann aber die direkte Kontrolle, wo die Daten dann landen. Wichtig ist dann auch, welche Datentypen diese Daten beinhalten (z.B. Tabellen, Text, Nummern, Geokoordinaten, etc.). Das wäre dann schon wichtig, dass die jeweiligen Daten ins entsprechende Feld ankommen, damit z.B. Geokoordinaten suchbar sind oder es sich nach Zeiträumen suchen lässt. Wenn man nur Text importiert, geht das natürlich nicht. Deshalb macht es wirklich Sinn, mit dem Schema zu arbeiten. 

### Suchindex vs. Datenbank
Bei einem Suchindex wie z.B. Solr oder einer Datenbank wie z.B. MySQL handelt es sich in beiden Fällen um Systeme, in dem Daten gespeichert und gezielt wieder rausgeholt werden. Aber die beiden Systeme haben unterschiedliche Schwerpunkte. 

So ist die Datenstruktur bei Suchindexen vergleichsweise eher flach. Jedes Objekt, das ich indexiere, ist genau ein Dokument mit verschiedenen Eigenschaften und Metadaten, aber es hat keine Beziehung zu anderen Objekten und steht alleine. Bei relationalen Datenbanken ist das - wie der Name schon sagt - anders. Hier gibt es Relationen und Beziehungen zwischen Objekten, was die Daten insgesamt viel strukturierter macht. 

Auch die Datenabfrage selbst läuft anders ab. Wie bereits erwähnt funktioniert Solr mit einer lexikalischen Suche. Der Suchbegriff selbst hat also Grammatik, während dem Suchprozess wird dieser aber auf seine Grundform reduziert und nach dieser Grundform wird dann auch gesucht. Bei Datenbanken werden via SQL-Abfragen bestimmte Datensätze abgefragt. Aber anders als bei der lexikalischen Suche vergleicht die Datenbank ihre Daten nur 1:1 mit dem Suchbegriff. 

Ein Suchindex hat weiter keine Kontrolle darüber, ob die darin enthaltenen Daten korrekt sind. Daten persistent zu sichern ist da nicht gut möglich, man speichert sie nur flüchtig. Bei einer Datenbank ist das anders. Die internen Datenbankregeln garantieren, dass der Datensatz auch im Falle eines Absturzes immer noch konsistent ist. In so einem Fall bleibt die Konsistenz der übrigen Datensätze erhalten, einfach ist derjenige, an dem man gerade gearbeitet hat, ist nach einem Absturz eventuell nicht mehr vorhanden.  

### Experimentieren mit VuFind
[folgt nocht]
